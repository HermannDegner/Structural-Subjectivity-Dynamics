# 構造主観力学における整合の数理モデル（拡張版）

## 1. はじめに

構造主観力学（SSD）は、世界のあらゆる事象を「構造」と「意味圧」の相互作用として捉えます。その中でも**「整合」**、すなわち構造が意味圧に対して安定を保とうとするプロセスは、学習、快・不快、疲労といった人間心理の根幹をなす現象と深く結びついています。

本稿では、この複雑な「整合」の力学を、電気回路のオーム則をアナロジーとした、観測可能で定量的な数理モデルとして記述する試みを解説します。

**拡張版の特徴：** 本モデルに**対数整合（Log-Alignment）**のオプションを追加し、Weber–Fechnerの法則に従う非線形応答を標準的に扱えるようにしています。これにより、広範な動的範囲での安定性が向上します。

---

## 1a. 対数整合の理論的背景

### Weber–Fechnerの法則

心理物理学における基本法則：**感覚の強度は刺激の対数に比例する**

$$S = k \log(I/I_0)$$

- $S$：感覚強度（主観的）
- $I$：物理的刺激強度
- $I_0$：閾値
- $k$：定数

### SSDにおける解釈

構造主観力学では、この法則を以下のように解釈します：

- **物理的刺激** $I$ → **意味圧** $p$
- **感覚強度** $S$ → **内部処理される意味圧** $\hat{p}$
- **閾値** $I_0$ → **適応レベル**（EMAで動的に推定）

つまり、構造は**非線形な世界を線形に"扱う"**ための前処理として、対数変換を自然に実装していると考えられます。

### 利点

1. **飽和防止**：極端な入力でも有限応答
2. **相対感度**：変化率に敏感（絶対値ではなく）
3. **動的範囲の拡大**：微弱～強大な刺激を統一的に処理
4. **環境適応**：明暗順応・音量順応などの自動調整

---

## 2. モデルの基本構成：オーム則のアナロジー

整合のプロセスを、電圧・電流・抵抗の関係になぞらえてモデル化します。

### 基本変数の定義

- **意味圧**: $p(t)$ - 構造に作用する入力や課題（電圧に相当）
- **整合（反応）**: $j(t)$ - 意味圧に対する構造の応答（電流に相当）
- **動きにくさ**: $R(t)$ - 整合の困難さやコスト（抵抗に相当）
- **通りやすさ**: $G(t) = \frac{1}{R(t)}$ - 整合の容易さ（コンダクタンスに相当）

### 基本関係式

$$j(t) = G(t) \cdot p(t) = \frac{p(t)}{R(t)}$$

この単純な関係式が全てのモデルの出発点となります。

---

## 3. モデルA：動的抵抗モデル（学習と疲労の分離）

このモデルでは、「抵抗 $R(t)$」を、性質の異なる二つの要素の和として捉えます。

$$R(t) = R_{\text{slow}}(t) + R_{\text{fast}}(t)$$

### 3.0. 対数整合との関係

動的抵抗モデルは**物理的抵抗の変化**に着目しますが、対数整合は**入力の知覚変換**に着目します。両者は独立に適用可能で、組み合わせることで：

- **対数層**：広範な入力を扱いやすい範囲に正規化
- **動的抵抗**：その範囲内での学習・疲労過程をモデル化

特に、対数整合の適応ゲイン $\alpha_t$ は、疲労状態 $R_{\text{fast}}$ と相関する可能性があります（高疲労時は感度が低下）。

### 3.1. 学習による抵抗変化 $R_{\text{slow}}$（可塑性・長期変化）

$R_{\text{slow}}$ は、学習や習慣化によってゆっくりと変化する、構造の根本的な「通りやすさ」を表します。

$$\frac{dR_{\text{slow}}}{dt} = -\eta \cdot S(t) + \lambda \cdot (R_\infty - R_{\text{slow}})$$

**パラメータ説明：**
- $\eta > 0$：学習率（整合成功時の抵抗減少率）
- $\lambda > 0$：忘却率（未使用時の抵抗増加率）
- $S(t)$：整合の成功度合い
- $R_\infty$：基準抵抗値

#### 学習効果
整合に成功する（$S(t) > 0$）と、その量に応じて抵抗が減少します。これが**「学習すれば楽になる」**状態です。

#### 忘却効果
整合が行われない期間が続くと、抵抗は元の基準値に向かって増加します。これは**「使わないと忘れる」**状態を表します。

### 3.2. 疲労による抵抗変化 $R_{\text{fast}}$（熱負荷・短期変化）

$R_{\text{fast}}$ は、高負荷な活動によって一時的に増大し、休息によって回復する抵抗成分です。

$$\frac{dR_{\text{fast}}}{dt} = \kappa \cdot P(t) - \frac{1}{\tau_{\text{rec}}} \cdot R_{\text{fast}}(t)$$

**パラメータ説明：**
- $\kappa > 0$：疲労蓄積係数
- $P(t) = j(t)^2 R(t)$：消費パワー
- $\tau_{\text{rec}} > 0$：回復時定数

#### 疲労蓄積
整合に伴い消費されるエネルギー $P(t)$ に比例して、抵抗が増加します。

#### 休息回復
負荷がかからない状態では、抵抗は指数関数的に減少します。

---

## 4. モデルB：慣性更新モデル（パワーバランス統合アプローチ）

### 4.1. 基本モデル（単線モデル）

#### 整合慣性 $\kappa$ と整合流 $j$

「通りやすさ」を整合慣性 $\kappa(t)$ で直接モデル化します。

$$j(t) = (G_0 + g \cdot \kappa(t)) \cdot p(t)$$

**パラメータ説明：**
- $G_0 > 0$：基本コンダクタンス
- $g > 0$：慣性の影響係数
- $\kappa(t) \geq \kappa_{\min}$：整合慣性

#### 4.1a. 対数整合拡張（Log-Alignment Layer）

非線形な意味圧を線形応答領域に写像する**Weber–Fechner対応**を導入できます：

$$j(t) = (G_0 + g \cdot \kappa(t)) \cdot \hat{p}(t)$$

ここで、**対数変換された意味圧** $\hat{p}$ は：

$$\hat{p}_i(t) = \operatorname{sign}(p_i) \cdot \frac{\log(1 + \alpha_t |p_i|)}{\log b}$$

**適応ゲイン**（明暗順応のアナロジー）：

$$\alpha_t = \frac{\alpha_0}{\epsilon + \mathrm{EMA}_\tau(|p|)}$$

**特徴：**
- 小信号域：$\hat{p} \approx \alpha_t p$（線形応答）
- 大信号域：感度が $\propto 1/|p|$ に減衰（飽和防止）
- $\mathrm{EMA}_\tau$：指数移動平均（時間定数 $\tau$）
- $b$：対数底（既定は $e$、デシベル解釈なら $10$）

> **使用判断**: 入力の動的範囲が広い場合（例：多様な課題難易度、環境刺激の変動）に有効。線形応答で十分な場合は $\hat{p} = p$ として標準モデルを使用。

#### パワーバランスと慣性更新

整合仕事を以下のように定義します：

$$\dot{W}_{\text{align}}(t) = p(t)j(t) - \rho j(t)^2 - \sigma p(t)^2$$

**対数整合使用時**は、変換後の意味圧を使用：

$$\dot{W}_{\text{align}}(t) = \hat{p}(t)j(t) - \rho j(t)^2 - \sigma \hat{p}(t)^2$$

慣性の更新則：

$$\frac{d\kappa}{dt} = \eta \cdot \dot{W}_{\text{align}}(t) - \lambda \cdot (\kappa(t) - \kappa_{\min})$$

**パラメータ説明：**

- $\dot{W}_{\text{align}}(t)$：正味の整合仕事（「快」の源泉）
- $\rho, \sigma > 0$：熱損失係数
- $\eta > 0$：学習効率
- $\lambda > 0$：忘却率
- $\kappa_{\min} > 0$：慣性の下限値

### 4.2. 整合効率と慣性成長の関係

学習曲線の特性を分析できます：

- **初期（低効率）**：$\dot{W}_{\text{align}} > 0$ となりやすく、$\kappa$ が急成長
- **熟練期（高効率）**：$\dot{W}_{\text{align}} \approx 0$ となり、$\kappa$ の成長が鈍化

**命題**: 構造慣性の成長速度は、効率の向上に伴い緩やかになる。

---

## 5. モデルの拡張：複合的な力学

### 5.1. 回避整合慣性モデル

接近と回避の二重経路を考慮したモデル：

$$j_{\text{total}}(t) = j_{\text{approach}}(t) - j_{\text{avoidance}}(t)$$

$$j_{\text{approach}}(t) = (G_0^+ + g^+ \kappa_+(t)) \cdot p(t)$$

$$j_{\text{avoidance}}(t) = (G_0^- + g^- \kappa_-(t)) \cdot p(t)$$

**特徴：**
- $\kappa_+(t)$：接近整合慣性
- $\kappa_-(t)$：回避整合慣性  
- 回避が優位になると行動が阻害される

### 5.2. 漏れ流（バイパス）モデル

課題に無関係な経路への散逸を考慮：

$$P_{\text{in,total}} = p(t) \cdot (j_{\text{approach}}(t) + b(t))$$

$$\dot{Q}_{\text{heat}} = \rho j_{\text{approach}}^2 + \sigma_b b^2$$

**特徴：**
- $b(t)$：漏れ流（課題無関係な活動）
- エネルギーは消費されるが成果は上がらない
- 「手応えのない疲労」の原因

---

## 6. パラメータ設定ガイド

### 6.1. 基本パラメータ一覧

| 記号 | 既定値 | 範囲 | 物理的意味 |
|------|--------|------|-----------|
| $G_0$ | 0.5 | 0.1–1.0 | 基本通りやすさ |
| $g$ | 0.7 | 0.2–1.5 | 慣性の影響度 |
| $\eta$ | 0.3 | 0.1–0.5 | 学習効率 |
| $\lambda$ | 0.02 | 0.005–0.05 | 忘却率 |
| $\rho$ | 0.1 | 0.05–0.2 | 抵抗損失係数 |
| $\sigma$ | 0.05 | 0.01–0.1 | 入力損失係数 |
| $\kappa_{\min}$ | 0.1 | 0.05–0.2 | 慣性下限値 |

### 6.1a. 対数整合パラメータ（オプション）

| 記号 | 既定値 | 範囲 | 物理的意味 |
|------|--------|------|-----------|
| $\alpha_0$ | 1.0 | 0.5–2.0 | 原点傾き（小信号ゲイン） |
| $b$ | $e$ | $e$ or $10$ | 対数底（$e$:自然、$10$:dB） |
| $\tau$ | 50 | 20–200 | EMA時定数（適応速度） |
| $\epsilon$ | $10^{-6}$ | — | 数値安定化定数 |
| $\zeta$ | auto | 0.5–2.0 | スケール整合係数 |

**$\zeta$ の推定**（未処理圧計算用）：

$$\zeta \approx \|G_0 + g\kappa\|_{\text{op}} \cdot c_b$$

- $c_b = 1$ （$b = e$ の場合）
- 実務では起動後のEMAまたは学習で自動調整

### 6.2. チューニング指針

#### システムが硬直・単調な場合

- $G_0 \uparrow, g \uparrow$：基本的な通りやすさ向上
- $\eta \uparrow$：学習効率向上  
- $\lambda \downarrow$：忘却遅延

#### システムが不安定な場合

- $\rho \uparrow$：失敗ペナルティ強化
- $\sigma \uparrow$：入力負荷調整
- $\kappa_{\min} \uparrow$：最低安定性確保

#### 対数整合の適用判断

**使用を推奨：**

- 入力の動的範囲が広い（例：難易度が10倍以上変動）
- 飽和現象が顕著（高負荷時に反応が頭打ち）
- Weber–Fechnerの法則的な感度が観測される

**標準モデルで十分：**

- 入力範囲が比較的狭い
- 線形応答で現象を説明可能
- 計算コストを最小化したい場合

---

## 7. シミュレーション例

### 7.1. 学習曲線のモデル化

典型的な学習プロセス：

```python
# 初期条件
kappa_0 = kappa_min
p_const = 1.0

# 期待される時間発展
# t=0-100:   kappa急成長, j増加, 熱損失大
# t=100-300: kappa成長鈍化, j安定, 効率向上  
# t=300-:    kappa飽和, j高位安定, 熱損失小
```

### 7.2. 対数整合による飽和抑制

高負荷入力時の比較：

```python
# 標準モデル（線形）
p_high = 10.0  # 高負荷入力
j_linear = (G0 + g*kappa) * p_high  # 比例的に大きくなる

# 対数整合モデル
import numpy as np
alpha_t = alpha0 / (eps + EMA_p)
p_hat = np.sign(p_high) * np.log1p(alpha_t * abs(p_high)) / np.log(base)
j_log = (G0 + g*kappa) * p_hat  # 飽和が抑制される

# 結果：未処理圧Eの蓄積が緩和され、破綻リスク低下
```

### 7.3. 疲労・回復サイクル

作業負荷パターンの分析：

$$R_{\text{fast}}(t) = R_0 e^{-t/\tau_{\text{rec}}} + \kappa \int_0^t P(\tau) e^{-(t-\tau)/\tau_{\text{rec}}} d\tau$$

**応用：** 最適な作業・休息比の導出

### 7.4. 対数整合の実装例

```python
import numpy as np

class LogAlignmentLayer:
    """対数整合変換層（Weber-Fechner対応）"""
    
    def __init__(self, alpha0=1.0, base=np.e, ema_tau=50, eps=1e-6):
        self.alpha0 = alpha0
        self.base = base
        self.log_base = np.log(base)
        self.ema_tau = ema_tau
        self.eps = eps
        self.ema_state = 0.0
        
    def transform(self, p):
        """意味圧pを対数変換"""
        # EMA更新（適応ゲイン計算）
        beta = np.exp(-1.0 / max(1, self.ema_tau))
        p_norm = np.linalg.norm(p) if hasattr(p, '__len__') else abs(p)
        self.ema_state = beta * self.ema_state + (1 - beta) * p_norm
        
        # 適応ゲイン
        alpha_t = self.alpha0 / (self.eps + self.ema_state)
        
        # 符号保持対数変換
        p_hat = np.sign(p) * np.log1p(alpha_t * np.abs(p)) / self.log_base
        
        return p_hat, alpha_t

class AlignmentModel:
    """整合慣性モデル（対数整合対応版）"""
    
    def __init__(self, G0=0.5, g=0.7, kappa_min=0.1, 
                 eta=0.3, lambda_=0.02, rho=0.1, sigma=0.05,
                 use_log_align=False, **log_params):
        self.G0 = G0
        self.g = g
        self.kappa = kappa_min
        self.kappa_min = kappa_min
        self.eta = eta
        self.lambda_ = lambda_
        self.rho = rho
        self.sigma = sigma
        self.use_log_align = use_log_align
        
        if use_log_align:
            self.log_layer = LogAlignmentLayer(**log_params)
    
    def step(self, p, dt=1.0):
        """1ステップの更新"""
        # 対数整合変換（オプション）
        if self.use_log_align:
            p_hat, alpha_t = self.log_layer.transform(p)
        else:
            p_hat = p
            alpha_t = None
        
        # 整合流
        j = (self.G0 + self.g * self.kappa) * p_hat
        
        # パワーバランス（整合仕事）
        W_align = np.dot(p_hat, j) - self.rho * np.dot(j, j) - self.sigma * np.dot(p_hat, p_hat)
        
        # 慣性更新
        dkappa = self.eta * W_align - self.lambda_ * (self.kappa - self.kappa_min)
        self.kappa = max(self.kappa_min, self.kappa + dkappa * dt)
        
        return {
            'j': j,
            'kappa': self.kappa,
            'W_align': W_align,
            'alpha_t': alpha_t,
            'p_hat': p_hat
        }

# 使用例
model_standard = AlignmentModel(use_log_align=False)
model_log = AlignmentModel(use_log_align=True, alpha0=1.0, ema_tau=50)

# 高負荷入力のシミュレーション
p_high = np.array([10.0, 5.0])
result_std = model_standard.step(p_high)
result_log = model_log.step(p_high)

print(f"標準モデル: j_norm={np.linalg.norm(result_std['j']):.3f}")
print(f"対数モデル: j_norm={np.linalg.norm(result_log['j']):.3f}")
print(f"飽和抑制効果: {(1 - np.linalg.norm(result_log['j'])/np.linalg.norm(result_std['j']))*100:.1f}%")
```

---

## 8. 実践的モデリング手順

### 段階的アプローチ

**第1段階：** 単線モデル（4.1節）で基本挙動を把握

- KPI: 整合効率 $j/p$, 熱損失 $\dot{Q}_{\text{heat}}$

**第2段階：** 必要に応じて拡張

- **反応頭打ち** → 並列整合モデル
- **行動阻害** → 回避整合慣性モデル  
- **手応えなき疲労** → 漏れ流モデル
- **目的齟齬** → 多重構造分析

**第3段階：** 対数整合の検討

- 動的範囲が広い → 対数整合を導入
- 適応ゲイン $\alpha_t$ で自動調整
- $\zeta$ パラメータの最適化

---

## 9. モデル検証

### 9.1. 定性的妥当性チェック

- ✅ 練習効果（$\kappa$ 増大）
- ✅ 忘却現象（$\kappa$ 減衰）
- ✅ 疲労蓄積（$R_{\text{fast}}$ 増大）
- ✅ 達成感（$\dot{W}_{\text{align}} > 0$ 時）
- ✅ Weber–Fechner応答（対数整合使用時）
- ✅ 飽和抑制（高負荷時の破綻回避）

### 9.2. 定量的検証の方向性

- 学習曲線データとの fitting
- 疲労回復時間の実測比較
- 認知負荷理論との接続
- 個体差パラメータの同定
- 対数応答特性の心理物理学的検証

---

## 10. 結論

SSDの整合数理モデルは：

1. **オーム則の直感的理解** から出発
2. **学習・疲労・快不快** を統一的に記述
3. **回避・漏れ・多重衝突** まで拡張可能
4. **対数整合による非線形応答** を標準化
5. **Weber–Fechner則との整合性** を確保
6. **実装・シミュレーション** に直結

最も重要なのは、感情的・主観的な「整合」体験を、**客観的で測定可能な力学プロセス**として扱えるようにする点です。

対数整合拡張により、**非線形な世界を線形に"扱う"**整合プロセスが明示化され、広範な動的範囲での安定性が向上します。

これにより、教育・組織運営・個人成長の各分野で、より科学的で効果的なアプローチが可能になります。

---

## 関連概念

- [構造観照（テオーリア）](./構造観照.md)
- [意味圧とは](./意味圧とは.md)
- [整合とは何か](./整合とは何か.md)
- [快の力学](./快の力学.md)
- [人間モジュール](./人間モジュール.md)
- [整合駆動型探索モデル](./整合駆動型探索モデル.md)
- [対数整合対応モデル](./ssd：対数整合対応モデル_v_1_（log_alignment_拡張）.md)
